{
  "hash": "28b59a71ccf90afe077d39df8a720a03",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Пространственная и географически взвешенная регрессия\"\nsubtitle: \"Основы геоинформатики: лекция 9\"\ndate: 04/07/2023\ndate-format: long\nauthor: \"Самсонов Тимофей Евгеньевич\"\nexecute:\n  echo: false\n  freeze: true\nengine: knitr\nformat:\n  revealjs: \n    theme: [default, custom_small.scss]\n    margin: 0.2\n    slide-number: true\n    footer: \"Самсонов Т. Е. Основы геоинформатики: курс лекций для студентов географического факультета МГУ\"\n    header-includes: <link rel=\"stylesheet\" media=\"screen\" href=\"https://fontlibrary.org//face/pt-sans\" type=\"text/css\"/>\nbibliography: references.yaml\nmainfont: PT Sans\n---\n\n\n## Пространственная зависимость\n\n::: columns\n::: {.column width=\"65%\"}\n**Пространственная зависимость** проявляется в том, что значения величины в соседних единицах измерений оказываются связаны.\n\nУчет этого фактора позволяет значительно усилить качество регрессионных моделей.\n\n::: callout-note\n## Как это работает?\n\nНапример, уровень преступности можно прогнозировать не только по доходам населения в районе, но и по уровню преступности *в соседних районах*.\n:::\n:::\n\n::: {.column width=\"35%\"}\n![](images/dependency.svg){width=\"100%\"}\n:::\n:::\n\n## Исходные данные\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-1-1.png){width=1500}\n:::\n:::\n\n\n## Исходные данные\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=1500}\n:::\n:::\n\n\n## Исходные данные\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-3-1.png){width=1500}\n:::\n:::\n\n\n## Диаграмма рассеяния\n\nДиаграмма рассеяния показывает соотношение переменных\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-4-1.png){width=1500}\n:::\n:::\n\n\n## Линейная регрессия\n\nЛинейная регрессия дает аппроксимацию зависимости\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-5-1.png){width=1500}\n:::\n:::\n\n\nКоэффициент корреляции равен $-0.696$.\n\n## Линейная регрессия\n\nЛинейная регрессия дает аппроксимацию зависимости\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-6-1.png){width=1500}\n:::\n:::\n\n\nКоэффициент корреляции равен $-0.574$.\n\n## Линейная регрессия\n\nЛинейная регрессия позволяет найти зависимость вида\n\n$$\ny = \\sum_{j=0}^k \\beta_j x_j\n$$\n\nгде $x_0 = 1$, а остальные $x_j$ --- независимые переменные.\n\nНапример, для двух переменных:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n$$\n\nВ этом уравнении 3 неизвестных коэффициента. Для их нахождения требуется как минимум 3 измерения. Но обычно их больше, поэтому получится аппроксимация зависимости.\n\n## Линейная регрессия\n\nПусть исследуемый показатель, а также независимые переменные измерены в $n$ географических местоположениях.\n\nДля нахождения $\\beta$ составляют систему из $i=1...n$ уравнений вида\n\n$$\ny_i = \\sum_{j=0}^k \\beta_j x_{ij}\n$$\n\nНапример, для четырех измерений:\n\n$$\ny_1 = \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12}\\\\\ny_2 = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22}\\\\\ny_3 = \\beta_0 + \\beta_1 x_{31} + \\beta_2 x_{32}\\\\\ny_4 = \\beta_0 + \\beta_1 x_{41} + \\beta_2 x_{42}\\\\\n$$\n\n## Линейная регрессия\n\nДля решения систему уравнений их записывают в матричном виде:\n\n$$\n\\underbrace{\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\ny_4 \\\\\n\\end{bmatrix}}_{\\textbf y} =\n\\underbrace{\\begin{bmatrix}\n1 & x_{11} & x_{12} \\\\\n1 & x_{21} & x_{22} \\\\\n1 & x_{31} & x_{32} \\\\\n1 & x_{41} & x_{42} \\\\\n\\end{bmatrix}}_{\\textbf X}\n\\underbrace{\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}}_{\\boldsymbol \\beta}\n$$\n\nИли более компактно:\n\n$$\n\\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta\n$$\n\n## Линейная регрессия\n\nЕсли предположить, что система решена и коэффициенты $\\beta$ найдены, то в каждом измерении получается ошибка (остаток) :\n\n$$\n\\epsilon_i = y_i - \\sum_{j=0}^k \\beta_j x_{ij}\n$$\n\n**Метод наименьших квадратов** позволяет минимизировать сумму:\n\n$$\n\\sum_{i=1}^n \\epsilon_i^2 \\rightarrow \\min\n$$\n\nГауссом доказано, что минимум достигается решением:\n\n$$\n\\boldsymbol \\beta = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y\n$$\n\n## Линейная регрессия\n\nМодель линейной регрессии записывается как:\n\n$$\\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\epsilon,$$\n\nгде:\n\n-   $\\mathbf{y} = \\{y_1, y_2, ... y_n\\}$ --- вектор измерений зависимой переменной по $n$ объектам,\n\n-   $\\mathbf{X} = \\{x_{ij}\\}$ --- матрица размером $n \\times (k+1)$, состоящая из значений $k$ независимых переменных для $n$ объектов (плюс константа $1$).\n\n-   $\\boldsymbol\\beta$ --- вектор коэффициентов регрессии;\n\n-   $\\boldsymbol\\epsilon$ --- вектор случайных ошибок (остатков).\n\n## Линейная регрессия\n\nДля модели\n\n$$\n\\texttt{CRIME} = \\beta_0 + \\beta_1 \\texttt{INC} + \\beta_2 \\texttt{HOVAL}\n$$\n\nполучается следующая диагностика:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate Std. Error    Pr(>|t|)\n(Intercept) 68.6189611  4.7354861 9.21089e-19\nINC         -1.5973108  0.3341308 1.82896e-05\nHOVAL       -0.2739315  0.1031987 1.08745e-02\n```\n\n\n:::\n:::\n\n\nТ.е. модель принимает следующий вид:\n\n$$\n\\texttt{CRIME} = 68.619 -1.597~\\texttt{INC} - 0.274~\\texttt{HOVAL}\n$$\n\n## Линейная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-8-1.png){width=1500}\n:::\n:::\n\n\n## Линейная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=1500}\n:::\n:::\n\n\n## Линейная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=1500}\n:::\n:::\n\n\n## Остатки регрессии\n\n::: columns\n::: {.column width=\"50%\"}\n::: callout-important\n## Важно\n\nЕсли остатки от регрессии образуют пространственный рисунок, это значит, что независимых переменных недостаточно для предсказания исследуемой величины. Необходимо учитывать пространственную зависимость.\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-11-1.png){width=1500}\n:::\n:::\n\n:::\n:::\n\nПри анализе карт остатков регрессии обращают внимание на то, меняются ли они плавно по пространству, есть ли выраженный пространственный тренд и зависимость значений соседних единиц.\n\n## Пространственная автокорреляция\n\nПространственная автокорреляция [@hubert:1981]\n\n:   Для множества $S$, состоящего из $n$ географических единиц, пространственная автокорреляция есть соотношение между переменной, наблюдаемой в каждой из $n$ единиц и мерой географической близости, определенной для всех $n(n − 1)$ пар единиц из $S$.\n\n-   Пространственная автокорреляция является количественной мерой пространственной зависимости.\n\n-   Для ее вычисления необходимо формализовать понятие географического соседства: какие объекты будем считать соседними и что будет мерой их близости?\n\n## Географическое соседство\n\nДля площадных территориальных единиц часто используется **соседство по смежности**, которое использует касание границ:\n\n![](images/QueenRook.png){width=\"100%\"}\n\n**Правило ферзя**: хотя бы одна общая точка на границе.\\\n**Правило ладьи**: общий участок линии на границе.\n\n## Географическое соседство\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-12-1.png){width=1500}\n:::\n:::\n\n\n## Географическое соседство\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-13-1.png){width=1500}\n:::\n:::\n\n\n## Пространственные веса\n\n**Пространственные веса** характеризуют силу связи между объектами\n\n-   Если единицы не являются соседними (по выбранному правилу), то пространственный вес их связи будет равен нулю. Во всех остальных случаях веса будут ненулевыми.\n\n-   Бинарные веса: если связь есть, то ее вес равен единице ($1$), если нет --- нулю ($0$).\n\n-   Нормированные веса: вес $j$-й единицы по отношению к $i$-й равен $1/n_i$, где $n_i$ --- количество соседей у $i$.\n\n## Пространственные веса\n\n::: columns\n::: {.column width=\"50%\"}\n**Бинарные веса**\n\n![](images/binary.svg){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\n**Матрица весов** $\\mathbf W$\n\n$$\n\\begin{bmatrix}\n0 & 0 & \\color{green}{\\mathbf 1} & 0 & \\color{green}{\\mathbf 1} & \\color{green}{\\mathbf 1} & 0 \\\\\n0 & 0 & \\color{blue}{\\mathbf 1} & 0 & 0 & 0 & \\color{blue}{\\mathbf 1} \\\\\n0 & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & 0 & 0 & \\color{blue}{\\mathbf 1} \\\\\n\\color{red}{\\mathbf 1} & \\color{red}{\\mathbf 1} & \\color{red}{\\mathbf 1} & 0 & \\color{red}{\\mathbf 1} & 0 & 0 \\\\\n\\color{blue}{\\mathbf 1} & 0 & 0 & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} \\\\\n\\color{blue}{\\mathbf 1} & 0 & 0 & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & 0 & 0 \\\\\n0 & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & \\color{blue}{\\mathbf 1} & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n:::\n:::\n\n## Пространственные веса\n\n::: columns\n::: {.column width=\"50%\"}\n**Нормированные веса**\n\n![](images/weighted.svg){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\nМатрица весов $\\mathbf W$\n\n$$\n\\small\n\\begin{bmatrix}\n0 & 0 & \\color{green}{\\mathbf{0.33}} & 0 & \\color{green}{\\mathbf{0.33}} & \\color{green}{\\mathbf{0.33}} & 0 \\\\\n0 & 0 & \\color{blue}{\\mathbf{0.5}} & 0 & 0 & 0 & \\color{blue}{\\mathbf{0.5}} \\\\\n0 & \\color{blue}{\\mathbf{0.25}} & \\color{blue}{\\mathbf{0.25}} & \\color{blue}{\\mathbf{0.25}} & 0 & 0 & \\color{blue}{\\mathbf{0.25}} \\\\\n\\color{red}{\\mathbf{0.25}} & \\color{red}{\\mathbf{0.25}} & \\color{red}{\\mathbf{0.25}} & 0 & \\color{red}{\\mathbf{0.25}} & 0 & 0 \\\\\n\\color{blue}{\\mathbf{0.2}} & 0 & 0 & \\color{blue}{\\mathbf{0.2}} & \\color{blue}{\\mathbf{0.2}} & \\color{blue}{\\mathbf{0.2}} & \\color{blue}{\\mathbf{0.2}} \\\\\n\\color{blue}{\\mathbf{0.33}} & 0 & 0 & \\color{blue}{\\mathbf{0.33}} & \\color{blue}{\\mathbf{0.33}} & 0 & 0 \\\\\n0 & \\color{blue}{\\mathbf{0.33}} & \\color{blue}{\\mathbf{0.33}} & \\color{blue}{\\mathbf{0.33}} & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\normalsize\n$$\n:::\n:::\n\n## Пространственные веса\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-14-1.png){width=1200}\n:::\n:::\n\n\n## Пространственные веса\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=1200}\n:::\n:::\n\n\n## Коэффициент корреляции Пирсона\n\nКоэффициент корреляции Пирсона вычисляется как:\n\n$$r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar x)^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar y)^2}}$$ где:\n\n-   $X = \\{x_i\\}$ и $Y = \\{y_i\\}$ --- две выборки значений;\n\n-   $\\bar x$ и $\\bar y$ --- средние арифметические.\n\n::: callout-note\n## Линейная зависимость\n\nКоэффициент корреляции Пирсона показывает зависимость только для переменных, имеющих связь линейного характера\n:::\n\n## Индекс Морана\n\nАнализ пространственной автокорреляции осуществляется, как правило, путем вычисления индекса Морана (Moran's I) [@moran1950]:\n\n$$I = \\frac{n \\sum^n_{i=1} \\sum^n_{j=i} w_{ij} (y_i - \\bar y)(y_j - \\bar y)}{ \\Big[\\sum^n_{i=1} \\sum^n_{j=i} w_{ij}\\Big] \\Big[\\sum^n_{i=1} (y_i - \\bar y)^2\\Big]}$$\n\nгде:\n\n-   $n$ --- количество единиц,\n-   $w_{ij}$ --- вес пространственной связи между $i$-й и $j$-й единицей,\n-   $y_i$ --- значение в $i$-й единице,\n-   $\\bar y$ --- выборочное среднее по всем единицам\n\n## Индекс Морана (Moran's I)\n\nИндекс Морана для нормально распределенных данных лежит в диапазоне от $-1$ до $1$:\n\n-   $+1$ означает детерминированную прямую зависимость --- группировку схожих (низких или высоких) значений;\n\n-   $0$ означает абсолютно случайное распределение;\n\n-   $-1$ означает детерминированную обратную зависимость --- идеальное перемешивание низких и высоких значений, напоминающее шахматную доску.\n\n**Математическое ожидание** индекса Морана для случайных данных равно $E[I] = -1/(n-1)$\n\n## Индекс Морана\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-16-1.png){width=900}\n:::\n:::\n\n\nИндекс Морана равен $0.500$\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-17-1.png){width=900}\n:::\n:::\n\n\nИндекс Морана равен $0.222$\n:::\n:::\n\nПоскольку остатки регрессии по-прежнему автокоррелированы, можно сделать вывод о том, что независимые переменные не объясняют полностью величину преступности.\n\n## Пространственная регрессия\n\nЧтобы учесть пространственную автокорреляцию зависимой переменной, в модель линейной регрессии добавляется компонента **авторегрессии** $\\rho\\mathbf{Wy}$ [@anselin:1988]:\n\n$$\\mathbf{y} = \\underbrace{\\mathbf{X} \\mathbf{\\beta}}_{тренд} + \\underbrace{\\color{red}{\\rho\\mathbf{Wy}}}_{сигнал} +  \\underbrace{\\mathbf{\\epsilon}}_{шум},$$\n\n-   $\\rho$ --- коэффициент авторегрессии, отражающий вклад пространственной автокорреляции;\n\n-   $\\mathbf{W}$ --- матрица пространственных весов.\n\nПолученная модель называется **пространственной регрессией**.\\\nТренд, сигнал и шум называются **предикторами**.\n\n## Пространственная регрессия\n\nПространственную регрессию можно представить как обычную регрессию. Выполним преобразования:\n\n$$\n\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} +  \\mathbf{\\epsilon}\\\\\n\\mathbf{y} - \\rho\\mathbf{Wy} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\\\\n(\\mathbf{I} - \\rho\\mathbf{W})\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\\\\n\\color{red}{\\boxed{\\color{blue}{\\mathbf{y} = (\\mathbf{I} - \\rho\\mathbf{W})^{-1}\\mathbf{X}\\mathbf{\\beta} + (\\mathbf{I} - \\rho\\mathbf{W})^{-1}\\mathbf{\\epsilon}}}}\n$$ Коэффициенты $\\beta$ и $\\rho$ находятся по методу наименьших квадратов.\n\nДля нашего случая модель будет иметь вид:\n\n$$\n\\texttt{CRIME} = 45.603 -1.049~\\texttt{INC} - 0.266~\\texttt{HOVAL} + 0.423~W~\\texttt{CRIME}\n$$\n\n## Пространственная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-18-1.png){width=1500}\n:::\n:::\n\n\n## Пространственная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-19-1.png){width=1500}\n:::\n:::\n\n\n## Пространственная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-20-1.png){width=1500}\n:::\n:::\n\n\n## Пространственная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-21-1.png){width=1500}\n:::\n:::\n\n\n## Пространственная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-22-1.png){width=1500}\n:::\n:::\n\n\n## Остатки пространств. регрессии\n\n**Индекс Морана** для остатков пространств. регрессии равен $0.033$.\n\n::: columns\n::: {.column width=\"40%\"}\nАвтокорреляционная составляющая практически полностью учтена в модели пространственной регрессии. Предсказательная сила модели улучшена.\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-23-1.png){width=900}\n:::\n:::\n\n:::\n:::\n\n## Пространственная гетерогенность\n\n::: columns\n::: {.column width=\"60%\"}\n**Пространственная гетерогенность** проявляется в том, что зависимости между переменными меняются по пространству.\n\nУчет этого фактора позволяет значительно усилить качество регрессионных моделей.\n\n::: callout-note\n## Как это работает?\n\nНапример, стоимость недвижимости может по-разному реагировать на увеличение жилплощади и количества комнат в разных городских районах.\n:::\n:::\n\n::: {.column width=\"40%\"}\n![](images/heterogeneity.svg){width=\"100%\"}\n:::\n:::\n\n## Исходные данные\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-25-1.png){width=1200}\n:::\n:::\n\n\n## Обычная регрессия\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-26-1.png){width=1800}\n:::\n:::\n\n\nКоэффициент детерминации $R^2 = 0.1483$. Регрессионная модель:\n\n$$\n\\texttt{price} = 2319.2 +421.8~\\texttt{rooms}\n$$\n\n## Географич. взвешенная регрессия\n\nВ стандартной модели линейной регрессии параметры $\\beta$ предполагаются постоянными. Для $i$-й локации решение выглядит следующим образом:\n\n$$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + \\epsilon_i$$\n\nВ географически взвешенной регрессии (ГВР) параметры $\\beta$ определяются для каждой локации [@fotheringham:2002]:\n\n$$y_i = \\beta_{0i} + \\beta_{1i} x_{1i} + \\beta_{2i} x_{2i} + ... + \\beta_{ki} x_{ki} + \\epsilon_i$$\n\nВ этом случае область оценки параметров $\\mathbf{\\beta}$ ограничивается некой окрестностью точки $i$.\n\n## Весовая функция\n\nДалёкие точки должны иметь меньший вес при вычислении коэффициентов. Например, для *гауссовой* весовой функции:\n\n$$\nw_{ij} = \\operatorname{exp}\\{-\\frac{1}{2} (d_{ij}/h)^2\\},\n$$\n\n::: columns\n::: {.column width=\"50%\"}\n-   $w_{ij}$ --- вес, который будет иметь $j$-я точка при вычислении коэффициентов регрессии в $i$-й точке;\n\n-   $d_{ij}$ расстояние между ними;\n\n-   $h$ --- полоса пропускания\n:::\n\n::: {.column width=\"50%\"}\n![](images/gwr_weights.svg){width=\"100%\"}\n:::\n:::\n\n## Весовые функции\n\nВ случае фиксированной весовой функции окрестность всегда имеет фиксированную полосу пропускания:\n\n![](images/nbfixed.svg){width=\"100%\"}\n\n## Весовые функции\n\nВ случае адаптивной весовой функции полоса пропускания определяется $N$ ближайшими точками. Например для $N = 5$:\n\n![](images/nbvariable.svg){width=\"100%\"}\n\n## Модель ГВР\n\nВ случае модели ГВР получается множество коэффициентов регрессии. По ним можно узнать статистику\n\n\n::: {.cell}\n\n:::\n\n\n```         \n              Min.   1st Qu. Median  3rd Qu. Max.\n   Intercept  198.56 1785.17 2054.43 2361.79 3485.2\n   rooms     -409.97  471.77  524.01  650.52 1299.1\n   \n   Kernel function: gaussian \n   Fixed bandwidth: 1000 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used\n```\n\nКоэффициент детерминации $R^2 = 0.367$.\n\n## Коэффициенты ГВР\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-28-1.png){width=900}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_Regression_files/figure-revealjs/unnamed-chunk-29-1.png){width=900}\n:::\n:::\n\n:::\n:::\n\nПространственная картина распределения коэффициентов регрессии подтверждает гипотезу о гетерогенности.\n\n## Словарик\n\n::: columns\n::: {.column width=\"50%\" style=\"color: blue; text-align: end;\"}\nЛинейная регрессия\n\nМетод наименьших квадратов\n\nДиаграмма рассеяния\n\nОстатки регрессии\n\nПростр. автокорреляция\n\nПространственные соседи\n\nПространственные веса\n\nПространственная регрессия\n\nГеографически взвешенная регрессия (ГВР)\n\nПолоса пропускания\n:::\n\n::: {.column width=\"50%\" style=\"color: red\"}\nLinear regression\n\nLeast squares method\n\nScatterplot\n\nRegression residuals\n\nSpatial autocorrelation\n\nSpatial neighbours\n\nSpatial weights\n\nSpatial regression\n\nGeographically weighted regression (GWR)\n\nBandwidth\n:::\n:::\n\n## Библиография\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}